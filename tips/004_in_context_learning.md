## Tip #4: Show, Don't Just Tell - Use In-Context Learning


While clear instructions and structured outputs (Tip #3) are vital, sometimes you need to explicitly *show* the LLM what a good answer looks like, especially for tasks requiring specific formats, styles, or complex reasoning. This technique is called **In-Context Learning (ICL)** or **Few-Shot Prompting**.

**The Core Idea: Provide Examples in the Prompt**

Instead of just relying on instructions (zero-shot), you include examples of input/output pairs directly within the prompt itself. The LLM uses these "shots" to better understand the desired pattern for the *actual* query you provide in the same prompt.

**Static vs. Dynamic Examples:**

* **Static Few-Shot:** You hardcode a few fixed examples directly into your prompt template. This is simple and great for reinforcing consistent formats or common cases.
* **Dynamic Few-Shot:** You select relevant examples on-the-fly based on the current user query. This is more powerful as it provides tailored guidance.

**How Dynamic Examples Leverage Your Workflow:**

Dynamic ICL works best when you have a pool of known good interactions. Where do these come from? Your **instrumented and evaluated production data!**

1.  **Instrumentation (Tip #2):** Your Lilypad traces log the inputs (`query`) and outputs (`final_answer`) of your AI calls.
2.  **Evaluation (Upcoming Tips):** You annotate these traces in Lilypad (e.g., marking them 'pass' or 'fail', adding qualitative feedback).
3.  **Retrieval (The Key Step):** Before calling the LLM for a new query, you'd query your Lilypad annotations:
    * Find past interactions marked 'pass'.
    * Use a similarity technique (like BM25 keyword matching or vector embedding similarity) to find the past queries most similar to the current `user_query`.
    * Retrieve the corresponding successful `answer` for those similar queries.
4.  **Injection:** Pass these dynamically retrieved examples into your Mirascope prompt.

**Example (Using Mirascope to Inject Examples into the Prompt):**

Let's focus on how Mirascope facilitates including these examples (static or dynamic) in the prompt structure. Assume you have already retrieved the relevant examples (e.g., via the Lilypad/similarity process described above) into a list called `selected_examples`.

```python
import os
from mirascope import llm, prompt_template
from pydantic import BaseModel, Field
from typing import List
import bm25s

# Define structure for an example pair
class Example(BaseModel):
    query: str
    answer: str # Could also be a Pydantic model for structured I/O examples

    @classmethod
    def from_trace(cls, tr: dict) -> 'Example':
        return cls(query=tr['arg_values']['query'], answer=tr['output'])

# Define the expected final response structure
class Response(BaseModel):
    final_answer: str = Field(description="The final answer generated by the assistant.")

# Define the prompt template with a placeholder for examples
FEW_SHOT_PROMPT_TEMPLATE = """
SYSTEM: You are a helpful assistant. Follow the format of the examples provided if any.

--- START EXAMPLES ---
{examples_block}
--- END EXAMPLES ---

USER: {query}
"""

@lilypad.trace(versioning='automatic')
@llm.call("openai", model="gpt-4o-mini", response_model=Response)
@prompt_template(FEW_SHOT_PROMPT_TEMPLATE)
def generate_response_with_examples(query: str, examples: List[Example]):
    """
    Generates a response using Mirascope, incorporating provided examples.
    The 'examples' list would be populated by fetching/ranking data from Lilypad annotations.
    """
    # Clearly format each example
    examples_str = "\n\n".join(
        f"EXAMPLE:\nUSER: {ex.query}\nASSISTANT: {ex.answer}"
        for ex in examples
    )

    # Return the formatted block to inject into the prompt template
    # Mirascope automatically injects 'query' from the function arguments.
    return {"computed_fields": {"examples_block": examples_block}}


def _retrieve_examples(examples: list[Example], k: int) -> list[(Example, float)]:
    corpus_tokens = bm25s.tokenize([x.query for x in examples])
    retriever = bm25s.BM25()
    retriever.index(corpus_tokens)
    query_tokens = bm25s.tokenize(query)
    return retriever.retrieve(query_tokens, k=k)[0]


def get_examples(query: str, k: int = 3) -> list[Example]:
    client = lilypad.Client()
    annotations = client.projects.annotations.list(project_uuid=os.environ.get())
    examples = [Example.from_trace(x) for x in annotations if x['label'] == 'pass']
    return _retrieve_examples(examples, k)


user_query = "How do I update my payment method?"
examples = get_examples(query)

response_object: Response = generate_response_with_examples(
    query=user_query,
    examples=examples
)

# Access the structured response
print("Final LLM Answer:", response_object.final_answer)
```

**Why Use In-Context Learning?**

* **Improved Accuracy:** Helps the LLM better understand complex instructions or nuances.
* **Better Format Adherence:** Excellent for teaching the LLM to output responses in a specific structure (like JSON, markdown, or a custom format).
* **Style & Tone Matching:** Guides the LLM to adopt a desired writing style or tone.
* **Leverages Past Success:** Dynamic ICL uses your annotated production data (via Lilypad) to continuously provide relevant guidance, improving performance over time.

**The Takeaway:**

Don't just rely on instructions. **Show the LLM what you want by including examples (few-shot prompting) directly in your prompts.** Start with static examples for consistency, and explore dynamic examples retrieved from your Lilypad annotations for more tailored and adaptive guidance. Use Mirascope to easily format and manage these examples within your LLM calls.