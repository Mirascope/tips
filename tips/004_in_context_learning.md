## Effective AI Engineering: Show, Don't Just Tell - Use In-Context Learning

**Are you getting unpredictable or poorly formatted LLM outputs?** Clear instructions alone often aren't enough to get consistent, high-quality responses from language models.

Language models can be frustratingly inconsistent when given abstract instructions. Even with detailed requirements, they may still produce responses that vary in structure, formatting, or reasoning quality. This inconsistency creates a maintenance burden and erodes trust in your AI-powered features.

### The Problem

Many developers rely solely on instructional prompts, hoping the LLM will understand their requirements:

```python
# BEFORE: Instruction-Only Prompting
def generate_answer(query: str):
    prompt = f"""
    SYSTEM: You are a helpful assistant. Answer the user's question with accurate information.
    Provide a structured, detailed explanation with relevant examples.
    
    USER: {query}
    """
    
    # No examples provided - just hoping the LLM understands the instructions
    response = llm_service.completion(prompt)
    return response
```

**Why this approach falls short:**

- **Inconsistent Formatting:** Without examples, the LLM's interpretation of "structured" varies widely, making parsing difficult.
- **Quality Variance:** Abstract instructions like "detailed explanation" yield unpredictable levels of detail and reasoning.
- **Poor Handling of Edge Cases:** The LLM lacks concrete guidance for unusual or complex queries.
- **Format Drift Over Time:** Different model versions may interpret the same instructions differently.

### The Solution: In-Context Learning (Few-Shot Prompting)

A better approach is to show the LLM what you want by including examples directly in your prompts. This technique, called In-Context Learning (ICL), provides concrete demonstrations that the model can imitate.

```python
# AFTER: Few-Shot Prompting with Examples
from mirascope import llm, prompt_template
from pydantic import BaseModel, Field

# Define structure for examples
class Example(BaseModel):
    query: str
    answer: str

# Define expected response structure
class Response(BaseModel):
    final_answer: str = Field(description="The final answer generated by the assistant.")

# Define prompt template with examples section
FEW_SHOT_PROMPT = """
SYSTEM: You are a helpful assistant. Follow the format of the examples provided.

--- START EXAMPLES ---
{examples_block}
--- END EXAMPLES ---

USER: {query}
"""

@llm.call("openai", model="gpt-4o-mini", response_model=Response)
@prompt_template(FEW_SHOT_PROMPT)
def generate_answer_with_examples(query: str, examples: list[Example]):
    """Generates a response using provided examples for guidance."""
    # Format the examples for insertion into the prompt
    examples_block = "\n\n".join(
        f"EXAMPLE:\nUSER: {ex.query}\nASSISTANT: {ex.answer}"
        for ex in examples
    )
    
    return {"computed_fields": {"examples_block": examples_block}}

# Use the function with selected examples
examples = [
    Example(query="How do solar panels work?", 
            answer="Solar panels work through the photovoltaic effect. When sunlight hits the semiconductor materials in the panel, it knocks electrons loose, generating electricity. This direct current (DC) is then converted to alternating current (AC) for home use."),
    # Add more examples...
]

response = generate_answer_with_examples(
    query="How do wind turbines generate electricity?",
    examples=examples
)
```

**Why this approach works better:**

- **Consistent Formatting:** Examples demonstrate the exact response structure you want, leading to more predictable outputs.
- **Improved Reasoning:** Examples show the level of detail and reasoning style expected, producing higher quality answers.
- **Better Edge Case Handling:** Examples can demonstrate how to address unusual situations or special cases.
- **Adaptable Guidance:** You can provide static examples for consistency or dynamically select relevant examples based on the query.

### Advanced Implementation: Dynamic Few-Shot Selection

For even better results, use your instrumented production data (Tip #2) to dynamically select the most relevant examples:

```python
# Building on previous example
import os
from lilypad import Client
import bm25s

def retrieve_examples(query: str, k: int = 3) -> list[Example]:
    # Connect to your instrumentation system
    client = Client()
    
    # Get successfully evaluated past interactions
    traces = client.projects.traces.list(project_uuid=os.environ.get("PROJECT_ID"))
    success_examples = [
        Example(query=tr['arg_values']['query'], answer=tr['output'])
        for tr in traces if tr.get('annotation', {}).get('label') == 'pass'
    ]
    
    # Use similarity search to find relevant examples
    corpus_tokens = bm25s.tokenize([ex.query for ex in success_examples])
    retriever = bm25s.BM25()
    retriever.index(corpus_tokens)
    query_tokens = bm25s.tokenize(query)
    
    # Get top-k most similar examples based on BM25
    # See later tips on retrieval and ranking for more thoughts on how to select examples!
    results = retriever.retrieve(query_tokens, k=k)[0]
    return [success_examples[idx] for idx, _ in results]
```

### The Takeaway

Don't just tell your LLM what to do â€“ show it with examples. In-Context Learning produces more consistent, higher-quality responses by demonstrating exactly what good outputs look like. This approach leads to more reliable AI systems that deliver predictable results and require less exception handling.

---
*Part of the "Effective AI Engineering" series - practical tips for building better applications with AI components.*