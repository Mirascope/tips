## Effective AI Engineering #xxx: Temperature Warming for Reliable LLM Responses

**Are you tired of your LLMs getting stuck in failure patterns?** When your model repeatedly fails with the same error despite multiple retries, a standard retry mechanism just wastes compute and time.

Even with well-crafted prompts, LLMs occasionally produce unusable outputs. Traditional retry approaches simply repeat the same request with identical parameters, causing models to get trapped in the same failure modes rather than finding alternative solution paths.

### The Problem

Many developers implement basic retry strategies that repeat the exact same request:

```python
# BEFORE: Simple Retry Strategy
from mirascope import anthropic, prompt_template
from tenacity import retry, stop_after_attempt
from pydantic import BaseModel, AfterValidator
from typing import Annotated

def is_upper(s: str) -> bool:
    return s.isupper()

@retry(stop=stop_after_attempt(3))
@anthropic.call(
    response_model=Annotated[str, AfterValidator(is_upper)],
)
def identify_author(book: str) -> str:
    return f"Who wrote {book}?"
```

**Why this approach falls short:**

- **Repeated Failures:** When using temperature=0, the model will produce the same output given the same input.
- **Wasted Computation:** Each retry costs tokens but doesn't improve success probability.
- **Fixed Solution Paths:** The model gets stuck in the same approach that led to validation failures.
- **Limited Exploration:** Without varying parameters, the model can't try alternative strategies.

### The Solution: Temperature Warming

A better approach is to implement temperature warming: incrementally increasing temperature with each retry. This gradually introduces randomness to help the model explore different solution paths when previous attempts fail.

```python
# AFTER: Temperature Warming Strategy
from mirascope import anthropic, prompt_template
from tenacity import retry, stop_after_attempt
from pydantic import BaseModel, AfterValidator, ValidationError
from typing import Annotated, Optional

def is_upper(s: str) -> bool:
    return s.isupper()

def collect_attempts(retry_state):
    attempt = retry_state.attempt_number
    print(f"Attempt {attempt} failed. Trying with higher temperature.")
    return attempt

@retry(stop=stop_after_attempt(3), after=collect_attempts)
@anthropic.call(
    response_model=Annotated[str, AfterValidator(is_upper)],
)
def identify_author(book: str, *, attempt: Optional[int] = None) -> str:
    if attempt is not None:
        # Gradually increase temperature from 0 to 0.7
        temperature = min(0.7, (attempt - 1) * 0.35)
        print(f"Using temperature: {temperature}")
        return {"call_params": {"temperature": temperature}}
    
    return f"Who wrote {book}?"
```

**Why this approach works better:**

- **Escape Validation Traps:** Higher temperatures help the model break free from recurring errors.
- **Progressive Exploration:** Starts deterministic, gradually becoming more exploratory when needed.
- **Minimal Implementation:** Simple to integrate with existing retry mechanisms.
- **Research-Validated:** Performs comparably to more complex retry architectures.

### The Takeaway

Temperature warming provides a simple yet powerful strategy for improving LLM reliability. By gradually increasing randomness on retries, you can significantly boost success rates without complex architectural changes or extensive prompt reengineering.

> **Reference:** This technique was highlighted in research summarized on [AI Snake Oil](https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful), where Perplexity AI researchers found that warming strategies performed comparably to complex agent-based approaches.

---
*Part of the "Effective AI Engineering" series - practical tips for building better applications with AI components.*